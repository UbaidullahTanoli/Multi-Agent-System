{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1tOFxJmAwat9dlbaf2n8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UbaidullahTanoli/Multi-Agent-System/blob/main/LangGraph_Multi_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1.   Install Packages**\n",
        "\n",
        "**2.   Import Libraries**\n",
        "\n",
        "**3.   Create Agent**\n",
        "\n",
        "**4.   Define Tools**\n",
        "\n",
        "**5.   Create Graph**\n",
        "\n",
        "*   **5A: Define State**\n",
        "*   **5B: Agent Node**\n",
        "*   **5C: Instantiate LLM**\n",
        "*   **5D: Define Tool Node**\n",
        "*   **5E: Define Edge Logic**\n",
        "*   **5F: Define Graph**\n",
        "\n",
        "\n",
        "**6.   Prompt and Invoke**"
      ],
      "metadata": {
        "id": "5kiPXXWcwhOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install Packages**"
      ],
      "metadata": {
        "id": "gH2kVG_42NJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib langgraph langchain_core\n",
        "!pip install langchain-huggingface text-generation google-search-results numexpr langchainhub sentencepiece jinja2\n",
        "%pip install -qU langchain-cohere"
      ],
      "metadata": {
        "id": "QE_i0_hxwIkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Import Libraries**"
      ],
      "metadata": {
        "id": "5gkrbS5X2Xo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import getpass\n",
        "import os\n",
        "import openai\n",
        "from langchain_core.messages import (\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from typing import Annotated\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "from langchain_openai import ChatOpenAI\n",
        "import functools\n",
        "from langchain_core.messages import AIMessage\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "#def _set_if_undefined(var: str):\n",
        "#    if not os.environ.get(var):\n",
        "#        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
        "\n",
        "\n",
        "#_set_if_undefined(\"OPENAI_API_KEY\")\n",
        "#_set_if_undefined(\"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\")\n",
        "#_set_if_undefined(\"TAVILY_API_KEY\")\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# Setting the SerpAPI key as an environment variable\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"598c99d847d8b4fbe441722ac89fdc2089ea90154f53dc34c4cc8d1ac171f7e8\"\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"]= \"sk-None-NhQwpcwXnHN6mhzYa6JaT3BlbkFJDX1E7LYAFhOtWO98Y5C8\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-BiqsUHCef8lQyzylfAviT3BlbkFJVGHTyMS9MFexj7C6u9Lw\"\n",
        "\n",
        "#os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API key: \")\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"DNku4TB3OmnAD86hQrtgXbuKCt6gdprF7xYU0upE\"\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAO87yDJBHCO45_HOQQnvqYeSQ3Drs3sM4\"\n",
        "\n",
        "os.environ[\"FIREWORKS_API_KEY\"] = \"FvbQCUd5fCtUOQ82X3U3tF8abGgQPGuqUy1HwDGjVueEJ6Hb\"\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"O2HAW9MUaG8UT1j4lVtLGKz8dNiSUmQu\""
      ],
      "metadata": {
        "id": "e_6UFk9V3axD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **3. Create Agent**"
      ],
      "metadata": {
        "id": "4Ybw3AqUuoqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent(llm, tools, system_message: str):\n",
        "    \"\"\"Create an agent.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "                \" Use the provided tools to progress towards answering the question.\"\n",
        "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "                \" will help where you left off. Execute what you can to make progress.\"\n",
        "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        ]\n",
        "    )\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
        "    return prompt | llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "xnEqE7BS5Z7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Define Tools**"
      ],
      "metadata": {
        "id": "XJdVAMROu2nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tavily_tool = TavilySearchResults(max_results=5)\n",
        "# Loading the tools\n",
        "#tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "tavily_tool = TavilySearchResults(max_results=3)\n",
        "\n",
        "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
        "\n",
        "repl = PythonREPL()\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_repl(\n",
        "    code: Annotated[str, \"The python code to execute to generate desired result.\"],\n",
        "):\n",
        "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
        "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
        "    try:\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        return f\"Failed to execute. Error: {repr(e)}\"\n",
        "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
        "    return (\n",
        "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "tOsnI4fb6Gr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Create Graph**\n",
        "\n"
      ],
      "metadata": {
        "id": "9CvppmNLvSNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5A: Define State**"
      ],
      "metadata": {
        "id": "ZEjqxeiUz4cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This defines the object that is passed between each node\n",
        "# in the graph. We will create different nodes for each agent and tool\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    sender: str"
      ],
      "metadata": {
        "id": "Si9vu7T67oCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5B: Define Agent Node**"
      ],
      "metadata": {
        "id": "M-DGrjN5z9_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to create a node for a given agent\n",
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    # We convert the agent output into a format that is suitable to append to the global state\n",
        "    if isinstance(result, ToolMessage):\n",
        "        pass\n",
        "    else:\n",
        "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
        "    return {\n",
        "        \"messages\": [result],\n",
        "        # Since we have a strict workflow, we can\n",
        "        # track the sender so we know who to pass to next.\n",
        "        \"sender\": name,\n",
        "    }"
      ],
      "metadata": {
        "id": "N98YN4nh7ztq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5C: Instantiate LLM**"
      ],
      "metadata": {
        "id": "u5gBmwK90Ns9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate llm\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/gemma-7b\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\":250, 'max_new_tokens': 200, 'top_k': 10, 'top_p': 0.95, 'repetition_penalty':1},\n",
        ")\n",
        "\n",
        "#llm = HuggingFaceEndpoint(\n",
        "#    repo_id=\"google/gemma-7b\",\n",
        "#    max_length=128,\n",
        "#    max_new_tokens = 100,\n",
        "#    temperature=0.5,\n",
        "#)\n",
        "\n",
        "\n",
        "#llm = ChatCohere(model = \"google/gemma-7b\")\n",
        "\n",
        "#model = \"google/gemma-7b\"\n",
        "\n",
        "#llm = ChatCohere(\n",
        "#    model=model,\n",
        "#    temperature=0,\n",
        "#    max_tokens=None,\n",
        "#    timeout=None,\n",
        "#    max_retries=2,\n",
        "    # other params...\n",
        "#)\n",
        "\n",
        "#llm = ChatOpenAI(base_url=\"https://api-inference.huggingface.co/v1\",\\\n",
        "#                 model=\"google/gemma-2b-it\",temperature=0.05)\n",
        "\n",
        "#llm = ChatOpenAI(base_url=\"https://api-inference.huggingface.co/v1\",api_key=\"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\",\\\n",
        "                # model=\"google/gemma-2b-it\",temperature=0.05)\n",
        "\n",
        "\n",
        "      # Used Google Cloud\n",
        "\"\"\"\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "PROJECT_ID = \"langgraph-multi-agent-system\"\n",
        "aiplatform.init(project=PROJECT_ID, location=\"europe-west9\")\n",
        "\n",
        "llm = ChatVertexAI(model=\"google/gemma-7b\")\n",
        "\"\"\"\n",
        "# A 10 pager error\n",
        "\n",
        "    # Using Fireworks\n",
        "\"\"\"\n",
        "from langchain_fireworks import ChatFireworks\n",
        "\n",
        "llm = ChatFireworks(model=\"google/gemma-7b\")\n",
        "\n",
        "#llm = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
        "\"\"\"\n",
        "# Error: Model not found\n",
        "\n",
        "  # Using Mistral AI\n",
        "\"\"\"\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "llm = ChatMistralAI(model=\"google/gemma-7b\")\n",
        "\"\"\"\n",
        "# Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {\"object\":\"error\",\"message\":\"Invalid model: google/gemma-7b\",\"type\":\"invalid_model\",\"param\":null,\"code\":\"1500\"}\n",
        "\n",
        "# Research agent and node\n",
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    [tavily_tool],\n",
        "    system_message=\"You should provide accurate data and information.\",\n",
        ")\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "# code_generator\n",
        "code_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl],\n",
        "    system_message=\"Any code you write will be visible by the user.\",\n",
        ")\n",
        "code_node = functools.partial(agent_node, agent=code_agent, name=\"code_generator\")"
      ],
      "metadata": {
        "id": "uH2a1lTQ8QQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "a4a51343-04d7-4c57-9692-30dd5d8e9b1c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HuggingFaceHub' object has no attribute 'bind_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-2bd9513922cc>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Research agent and node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m research_agent = create_agent(\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtavily_tool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3f5fb5a0a405>\u001b[0m in \u001b[0;36mcreate_agent\u001b[0;34m(llm, tools, system_message)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtool\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceHub' object has no attribute 'bind_tools'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5D: Define Tool Node**"
      ],
      "metadata": {
        "id": "Ote-PMnl0TWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [tavily_tool, python_repl]\n",
        "tool_node = ToolNode(tools)"
      ],
      "metadata": {
        "id": "OXLrn3ciCTDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5E: Define Edge Logic**"
      ],
      "metadata": {
        "id": "luMRD5BR0YZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Either agent can decide to end\n",
        "\n",
        "def router(state):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    if \"function_call\" in last_message.additional_kwargs:\n",
        "        # The previus agent is invoking a tool\n",
        "        return \"call_tool\"\n",
        "\n",
        "    if \"FINAL ANSWER\" in last_message.content:\n",
        "        # Any agent decided the work is done\n",
        "        return \"end\"\n",
        "\n",
        "    return \"continue\""
      ],
      "metadata": {
        "id": "0d1As0DHCma9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5F: Define Graph**"
      ],
      "metadata": {
        "id": "5Ddgj7uC0e9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"code_generator\", code_node)\n",
        "workflow.add_node(\"call_tool\", tool_node)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"Researcher\",\n",
        "    router,\n",
        "    {\"continue\": \"code_generator\", \"call_tool\": \"call_tool\", \"__end__\": END},\n",
        ")\n",
        "workflow.add_conditional_edges(\n",
        "    \"code_generator\",\n",
        "    router,\n",
        "    {\"continue\": \"Researcher\", \"call_tool\": \"call_tool\", \"__end__\": END},\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"call_tool\",\n",
        "    # Each agent node updates the 'sender' field\n",
        "    # the tool calling node does not, meaning\n",
        "    # this edge will route back to the original agent\n",
        "    # who invoked the tool\n",
        "    lambda x: x[\"sender\"],\n",
        "    {\n",
        "        \"Researcher\": \"Researcher\",\n",
        "        \"code_generator\": \"code_generator\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(START, \"Researcher\")\n",
        "#workflow.set_entry_point(\"Researcher\")\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "5WktBpuICv4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Prompt and Invoke**"
      ],
      "metadata": {
        "id": "B7Wae0cH0llX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for s in graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"What is LangChain?\"\n",
        "            )\n",
        "        ],\n",
        "    },\n",
        "    # Maximum number of steps to take in the graph\n",
        "    {\"recursion_limit\": 60},\n",
        "):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "rxPF7EhyDP7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}